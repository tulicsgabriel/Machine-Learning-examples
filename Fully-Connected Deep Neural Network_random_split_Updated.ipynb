{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Deep Neural Network with random split validation\n",
    "\n",
    "### My updated template for a binary classification, with a confusion matrix\n",
    "\n",
    "### Metrics given:\n",
    "\n",
    "Accuracy, Matthews Correlation Coefficient\n",
    "\n",
    "For each class: Recall, Precision, F1 score, Specificity, Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras import regularizers, losses\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from sklearn.impute import SimpleImputer as Imputer #class\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import keras_metrics\n",
    "from keras import optimizers\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "#from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler # https://jovianlin.io/feature-scaling/\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Csoport</th>\n",
       "      <th>Nem</th>\n",
       "      <th>SPI.raw_0_0</th>\n",
       "      <th>MFCC.mean_0_0_[E]_1</th>\n",
       "      <th>MFCC.std_0_0_[E]_1</th>\n",
       "      <th>MFCC.range_0_0_[E]_1</th>\n",
       "      <th>HNR.mean_5_5_[E]</th>\n",
       "      <th>HNR.std_5_5_[E]</th>\n",
       "      <th>HNR.range_5_5_[E]</th>\n",
       "      <th>...</th>\n",
       "      <th>IMF_ENTROPY_RATIO.range_0_0_[E-e:-i-2-y]</th>\n",
       "      <th>IMF_ENTROPY_RATIO.mean_0_0_[O-A:-o-u]</th>\n",
       "      <th>IMF_ENTROPY_RATIO.std_0_0_[O-A:-o-u]</th>\n",
       "      <th>IMF_ENTROPY_RATIO.range_0_0_[O-A:-o-u]</th>\n",
       "      <th>IMF_ENTROPY_RATIO.mean_0_0_[v-z-Z]</th>\n",
       "      <th>IMF_ENTROPY_RATIO.std_0_0_[v-z-Z]</th>\n",
       "      <th>IMF_ENTROPY_RATIO.range_0_0_[v-z-Z]</th>\n",
       "      <th>IMF_ENTROPY_RATIO.mean_0_0_[b-d-g-dz-dZ-d']</th>\n",
       "      <th>IMF_ENTROPY_RATIO.std_0_0_[b-d-g-dz-dZ-d']</th>\n",
       "      <th>IMF_ENTROPY_RATIO.range_0_0_[b-d-g-dz-dZ-d']</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PA_002no</td>\n",
       "      <td>PA</td>\n",
       "      <td>no</td>\n",
       "      <td>0.702423</td>\n",
       "      <td>226.727241</td>\n",
       "      <td>15.565275</td>\n",
       "      <td>138.304191</td>\n",
       "      <td>7.702671</td>\n",
       "      <td>1.873980</td>\n",
       "      <td>8.800351</td>\n",
       "      <td>...</td>\n",
       "      <td>2.379776</td>\n",
       "      <td>1.229107</td>\n",
       "      <td>0.297684</td>\n",
       "      <td>1.656664</td>\n",
       "      <td>1.342090</td>\n",
       "      <td>0.599020</td>\n",
       "      <td>2.269212</td>\n",
       "      <td>2.035604</td>\n",
       "      <td>0.885508</td>\n",
       "      <td>3.693024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PA_003no</td>\n",
       "      <td>PA</td>\n",
       "      <td>no</td>\n",
       "      <td>0.368337</td>\n",
       "      <td>188.198555</td>\n",
       "      <td>20.797340</td>\n",
       "      <td>127.246116</td>\n",
       "      <td>-3.235416</td>\n",
       "      <td>1.841550</td>\n",
       "      <td>6.213561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.950906</td>\n",
       "      <td>1.054856</td>\n",
       "      <td>0.154813</td>\n",
       "      <td>0.754372</td>\n",
       "      <td>1.022489</td>\n",
       "      <td>0.159252</td>\n",
       "      <td>0.522101</td>\n",
       "      <td>1.098897</td>\n",
       "      <td>0.111197</td>\n",
       "      <td>0.488349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PA_004no</td>\n",
       "      <td>PA</td>\n",
       "      <td>no</td>\n",
       "      <td>0.810142</td>\n",
       "      <td>245.252498</td>\n",
       "      <td>13.106253</td>\n",
       "      <td>94.247744</td>\n",
       "      <td>9.765168</td>\n",
       "      <td>1.553110</td>\n",
       "      <td>7.726234</td>\n",
       "      <td>...</td>\n",
       "      <td>2.337904</td>\n",
       "      <td>1.175937</td>\n",
       "      <td>0.285692</td>\n",
       "      <td>1.626280</td>\n",
       "      <td>1.212787</td>\n",
       "      <td>0.271315</td>\n",
       "      <td>0.985533</td>\n",
       "      <td>1.467220</td>\n",
       "      <td>0.332705</td>\n",
       "      <td>1.475147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PA_005no</td>\n",
       "      <td>PA</td>\n",
       "      <td>no</td>\n",
       "      <td>1.052086</td>\n",
       "      <td>271.537454</td>\n",
       "      <td>21.658928</td>\n",
       "      <td>129.618586</td>\n",
       "      <td>11.969402</td>\n",
       "      <td>1.212987</td>\n",
       "      <td>5.478905</td>\n",
       "      <td>...</td>\n",
       "      <td>2.311154</td>\n",
       "      <td>1.010327</td>\n",
       "      <td>0.270293</td>\n",
       "      <td>1.332628</td>\n",
       "      <td>1.857653</td>\n",
       "      <td>0.562689</td>\n",
       "      <td>2.146590</td>\n",
       "      <td>1.508461</td>\n",
       "      <td>0.403692</td>\n",
       "      <td>1.557925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PA_006no</td>\n",
       "      <td>PA</td>\n",
       "      <td>no</td>\n",
       "      <td>0.739211</td>\n",
       "      <td>249.913724</td>\n",
       "      <td>16.760158</td>\n",
       "      <td>117.845775</td>\n",
       "      <td>2.424641</td>\n",
       "      <td>4.498472</td>\n",
       "      <td>16.214943</td>\n",
       "      <td>...</td>\n",
       "      <td>3.031446</td>\n",
       "      <td>1.277202</td>\n",
       "      <td>0.288863</td>\n",
       "      <td>1.337109</td>\n",
       "      <td>1.138326</td>\n",
       "      <td>0.284632</td>\n",
       "      <td>1.042373</td>\n",
       "      <td>1.441762</td>\n",
       "      <td>0.329920</td>\n",
       "      <td>1.256846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Csoport Nem  SPI.raw_0_0  MFCC.mean_0_0_[E]_1  MFCC.std_0_0_[E]_1  \\\n",
       "0  PA_002no      PA  no     0.702423           226.727241           15.565275   \n",
       "1  PA_003no      PA  no     0.368337           188.198555           20.797340   \n",
       "2  PA_004no      PA  no     0.810142           245.252498           13.106253   \n",
       "3  PA_005no      PA  no     1.052086           271.537454           21.658928   \n",
       "4  PA_006no      PA  no     0.739211           249.913724           16.760158   \n",
       "\n",
       "   MFCC.range_0_0_[E]_1  HNR.mean_5_5_[E]  HNR.std_5_5_[E]  HNR.range_5_5_[E]  \\\n",
       "0            138.304191          7.702671         1.873980           8.800351   \n",
       "1            127.246116         -3.235416         1.841550           6.213561   \n",
       "2             94.247744          9.765168         1.553110           7.726234   \n",
       "3            129.618586         11.969402         1.212987           5.478905   \n",
       "4            117.845775          2.424641         4.498472          16.214943   \n",
       "\n",
       "   ...  IMF_ENTROPY_RATIO.range_0_0_[E-e:-i-2-y]  \\\n",
       "0  ...                                  2.379776   \n",
       "1  ...                                  0.950906   \n",
       "2  ...                                  2.337904   \n",
       "3  ...                                  2.311154   \n",
       "4  ...                                  3.031446   \n",
       "\n",
       "   IMF_ENTROPY_RATIO.mean_0_0_[O-A:-o-u]  \\\n",
       "0                               1.229107   \n",
       "1                               1.054856   \n",
       "2                               1.175937   \n",
       "3                               1.010327   \n",
       "4                               1.277202   \n",
       "\n",
       "   IMF_ENTROPY_RATIO.std_0_0_[O-A:-o-u]  \\\n",
       "0                              0.297684   \n",
       "1                              0.154813   \n",
       "2                              0.285692   \n",
       "3                              0.270293   \n",
       "4                              0.288863   \n",
       "\n",
       "   IMF_ENTROPY_RATIO.range_0_0_[O-A:-o-u]  IMF_ENTROPY_RATIO.mean_0_0_[v-z-Z]  \\\n",
       "0                                1.656664                            1.342090   \n",
       "1                                0.754372                            1.022489   \n",
       "2                                1.626280                            1.212787   \n",
       "3                                1.332628                            1.857653   \n",
       "4                                1.337109                            1.138326   \n",
       "\n",
       "   IMF_ENTROPY_RATIO.std_0_0_[v-z-Z]  IMF_ENTROPY_RATIO.range_0_0_[v-z-Z]  \\\n",
       "0                           0.599020                             2.269212   \n",
       "1                           0.159252                             0.522101   \n",
       "2                           0.271315                             0.985533   \n",
       "3                           0.562689                             2.146590   \n",
       "4                           0.284632                             1.042373   \n",
       "\n",
       "   IMF_ENTROPY_RATIO.mean_0_0_[b-d-g-dz-dZ-d']  \\\n",
       "0                                     2.035604   \n",
       "1                                     1.098897   \n",
       "2                                     1.467220   \n",
       "3                                     1.508461   \n",
       "4                                     1.441762   \n",
       "\n",
       "   IMF_ENTROPY_RATIO.std_0_0_[b-d-g-dz-dZ-d']  \\\n",
       "0                                    0.885508   \n",
       "1                                    0.111197   \n",
       "2                                    0.332705   \n",
       "3                                    0.403692   \n",
       "4                                    0.329920   \n",
       "\n",
       "   IMF_ENTROPY_RATIO.range_0_0_[b-d-g-dz-dZ-d']  \n",
       "0                                      3.693024  \n",
       "1                                      0.488349  \n",
       "2                                      1.475147  \n",
       "3                                      1.557925  \n",
       "4                                      1.256846  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the data\n",
    "dataset = pd.read_csv('HC_PA_data.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0\n",
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "..  ..\n",
      "445  1\n",
      "446  0\n",
      "447  0\n",
      "448  0\n",
      "449  1\n",
      "\n",
      "[450 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle\n",
    "np.random.seed(42) # random seed is a number (or vector) used to initialize a pseudorandom number generator\n",
    "dataset = dataset.reindex(np.random.permutation(dataset.index))\n",
    "dataset.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Selecting the training attributes(X) and the label(y)\n",
    "X = dataset.iloc[:, 3:52].values # X = dataset.iloc[:, np.r_[3:52]].values  lets you choose multiple coloumbs\n",
    "y = dataset.iloc[:,1].values\n",
    "\n",
    "# encode target\n",
    "encode = {\"HC\" : 0, \"PA\" : 1}\n",
    "decode = { 0 : \"HC\", 1 : \"PA\"}\n",
    "\n",
    "# Change target from string to binary value\n",
    "y = pd.DataFrame(y).replace(encode)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Change y to categorical\n",
    "from keras.utils import np_utils\n",
    "y_categorical = np_utils.to_categorical(y)\n",
    "print(y_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values in columns with columns mean values \n",
    "imputer = Imputer(missing_values=np.nan, strategy = 'mean') # an instance of the class with these properties\n",
    "imputer = imputer.fit(X)         # we have to choose the columns with missing values\n",
    "X = imputer.transform(X)           # replace the X values for the columns averages\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size = 0.25, random_state = 0) \n",
    "\n",
    "# Feature Scaling to 0, 1 range\n",
    "from sklearn.preprocessing import MinMaxScaler # Normalization x_norm = (x- min(x))/(max(x)-min(x))\n",
    "sc = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Sequential_DNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "first_hidden_layer (Dense)   (None, 49)                2450      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "second_hidden_layer (Dense)  (None, 25)                1250      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "final_layer (Dense)          (None, 2)                 52        \n",
      "=================================================================\n",
      "Total params: 3,752\n",
      "Trainable params: 3,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the network\n",
    "\n",
    "from keras.layers import InputLayer\n",
    "#print(X.shape[0])\n",
    "\n",
    "NUM_COLS = X.shape[1]\n",
    "NUM_ROWS = X.shape[0]\n",
    "#input_shape=(NUM_ROWS * NUM_COLS,)\n",
    "\n",
    "def get_sequential_dnn():\n",
    "    # create model\n",
    "    classifier = Sequential(name=\"Sequential_DNN\")  # future ANN classifier, now we initialize the different hidden layers \n",
    "    # ReLu activation function for the hidden layers\n",
    "    # Sigmoid for the final layer\n",
    "    \n",
    "    # input layer and the first hidden layer\n",
    "    classifier.add(InputLayer(input_shape=(X.shape[1])))\n",
    "    classifier.add(Dense(49, activation='relu', name=\"first_hidden_layer\", input_shape=(NUM_ROWS * NUM_COLS,)))\n",
    "    \n",
    "    \n",
    "    # tip: number of nodes in the hidden layers = average (number of nodes in the input layer and the number of nodes in the output layer) \n",
    "                                # so output_dim =(49+1)/2 = 25\n",
    "        \n",
    "                                # init -> initialize the weights randomly\n",
    "                                # input_dim = 49\n",
    "    \n",
    "    # Add a dropout layer for input layer\n",
    "    # More to read about dropout here: http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "    \n",
    "    classifier.add(Dropout(0.25))\n",
    "    \n",
    "    #Then we simply add the input-, hidden- and output-layers. \n",
    "    # Between them, we are using dropout to prevent overfitting. \n",
    "    # Note that you should always use a dropout rate between 20% and 50%. \n",
    "    # At every layer, we use “Dense” which means that the units are fully connected. \n",
    "    \n",
    "    # second hidden layer\n",
    "    #classifier.add(Dense(output_dim = 25, kernel_regularizer = regularizers.l2(0.01), activity_regularizer = regularizers.l1(0.01), init ='uniform', activation = 'relu', input_dim = 49))         \n",
    "    classifier.add(Dense(25, activation = 'relu', name=\"second_hidden_layer\"))\n",
    "    classifier.add(Dropout(0.25))\n",
    "    #classifier.add(Dropout(0.5))\n",
    "        \n",
    "    # third hidden layer\n",
    "    #classifier.add(Dense(25, activation = 'relu', name=\"third_hidden_layer\"))\n",
    "    #classifier.add(Dropout(0.25))\n",
    "    #classifier.add(Dropout(0.5))\n",
    "    \n",
    "    # 4th hidden layer\n",
    "    #classifier.add(Dense(25, activation = 'relu', name=\"forth_hidden_layer\"))\n",
    "    #classifier.add(Dropout(0.25))\n",
    "    #classifier.add(Dropout(0.5))\n",
    "    \n",
    "    #classifier.add(Dense(output_dim = 2, activation = 'softmax', name=\"output_layer\"))\n",
    "    classifier.add(Dense(2, activation='softmax', name=\"final_layer\"))\n",
    "    return classifier\n",
    "\n",
    "classifier=get_sequential_dnn()\n",
    "classifier.summary()\n",
    "#print(len(classifier.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /home/gabriel/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras_metrics/metrics.py:51: calling Layer.add_update (from tensorflow.python.keras.engine.base_layer) with inputs is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`inputs` is now automatically inferred\n",
      "WARNING:tensorflow:From /home/gabriel/anaconda3/envs/tensorflow/lib/python3.8/site-packages/keras_metrics/metrics.py:26: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.7194 - precision: 0.2849 - recall: 0.0565 - acc: 0.5460 - val_loss: 0.7028 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5487\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.7114 - precision: 0.3289 - recall: 0.1050 - acc: 0.5371 - val_loss: 0.6880 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_acc: 0.5487\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6893 - precision: 0.4229 - recall: 0.1786 - acc: 0.5549 - val_loss: 0.6791 - val_precision: 0.5000 - val_recall: 0.0196 - val_acc: 0.5487\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6733 - precision: 0.5408 - recall: 0.2563 - acc: 0.5964 - val_loss: 0.6729 - val_precision: 0.7500 - val_recall: 0.0588 - val_acc: 0.5664\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6767 - precision: 0.5064 - recall: 0.3159 - acc: 0.5846 - val_loss: 0.6664 - val_precision: 1.0000 - val_recall: 0.0588 - val_acc: 0.5752\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6725 - precision: 0.5027 - recall: 0.2549 - acc: 0.5816 - val_loss: 0.6611 - val_precision: 1.0000 - val_recall: 0.0980 - val_acc: 0.5929\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6703 - precision: 0.5393 - recall: 0.2661 - acc: 0.5905 - val_loss: 0.6555 - val_precision: 1.0000 - val_recall: 0.0980 - val_acc: 0.5929\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6596 - precision: 0.5298 - recall: 0.2120 - acc: 0.6083 - val_loss: 0.6491 - val_precision: 1.0000 - val_recall: 0.0784 - val_acc: 0.5841\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6467 - precision: 0.6566 - recall: 0.2741 - acc: 0.6350 - val_loss: 0.6411 - val_precision: 1.0000 - val_recall: 0.0980 - val_acc: 0.5929\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6402 - precision: 0.6957 - recall: 0.2874 - acc: 0.6291 - val_loss: 0.6321 - val_precision: 1.0000 - val_recall: 0.1373 - val_acc: 0.6106\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6288 - precision: 0.8028 - recall: 0.3412 - acc: 0.6825 - val_loss: 0.6229 - val_precision: 1.0000 - val_recall: 0.2157 - val_acc: 0.6460\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6389 - precision: 0.6089 - recall: 0.3332 - acc: 0.6291 - val_loss: 0.6146 - val_precision: 1.0000 - val_recall: 0.2549 - val_acc: 0.6637\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6240 - precision: 0.6106 - recall: 0.3696 - acc: 0.6558 - val_loss: 0.6067 - val_precision: 1.0000 - val_recall: 0.2941 - val_acc: 0.6814\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.6140 - precision: 0.6696 - recall: 0.3453 - acc: 0.6558 - val_loss: 0.5987 - val_precision: 1.0000 - val_recall: 0.3922 - val_acc: 0.7257\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.6236 - precision: 0.6932 - recall: 0.4133 - acc: 0.6469 - val_loss: 0.5908 - val_precision: 1.0000 - val_recall: 0.4314 - val_acc: 0.7434\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6123 - precision: 0.6797 - recall: 0.4151 - acc: 0.6766 - val_loss: 0.5848 - val_precision: 1.0000 - val_recall: 0.3922 - val_acc: 0.7257\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6114 - precision: 0.6718 - recall: 0.3432 - acc: 0.6588 - val_loss: 0.5790 - val_precision: 1.0000 - val_recall: 0.3922 - val_acc: 0.7257\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6095 - precision: 0.7150 - recall: 0.4191 - acc: 0.6884 - val_loss: 0.5728 - val_precision: 1.0000 - val_recall: 0.4118 - val_acc: 0.7345\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6101 - precision: 0.7347 - recall: 0.4507 - acc: 0.6914 - val_loss: 0.5620 - val_precision: 0.9231 - val_recall: 0.4706 - val_acc: 0.7434\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6043 - precision: 0.6732 - recall: 0.4539 - acc: 0.6795 - val_loss: 0.5528 - val_precision: 0.9091 - val_recall: 0.5882 - val_acc: 0.7876\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5940 - precision: 0.7114 - recall: 0.4704 - acc: 0.6855 - val_loss: 0.5466 - val_precision: 0.9118 - val_recall: 0.6078 - val_acc: 0.7965\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5893 - precision: 0.6906 - recall: 0.5682 - acc: 0.7003 - val_loss: 0.5428 - val_precision: 0.9310 - val_recall: 0.5294 - val_acc: 0.7699\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5838 - precision: 0.7011 - recall: 0.4867 - acc: 0.6825 - val_loss: 0.5353 - val_precision: 0.9032 - val_recall: 0.5490 - val_acc: 0.7699\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.5762 - precision: 0.6927 - recall: 0.5359 - acc: 0.7211 - val_loss: 0.5272 - val_precision: 0.9062 - val_recall: 0.5686 - val_acc: 0.7788\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5593 - precision: 0.7197 - recall: 0.5453 - acc: 0.7062 - val_loss: 0.5170 - val_precision: 0.8974 - val_recall: 0.6863 - val_acc: 0.8230\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5848 - precision: 0.6885 - recall: 0.5856 - acc: 0.7033 - val_loss: 0.5088 - val_precision: 0.8182 - val_recall: 0.7059 - val_acc: 0.7965\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5465 - precision: 0.7206 - recall: 0.6407 - acc: 0.7122 - val_loss: 0.5014 - val_precision: 0.8372 - val_recall: 0.7059 - val_acc: 0.8053\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5632 - precision: 0.6574 - recall: 0.5951 - acc: 0.7211 - val_loss: 0.4990 - val_precision: 0.9167 - val_recall: 0.6471 - val_acc: 0.8142\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5434 - precision: 0.6883 - recall: 0.5777 - acc: 0.7300 - val_loss: 0.4945 - val_precision: 0.9118 - val_recall: 0.6078 - val_acc: 0.7965\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5504 - precision: 0.7135 - recall: 0.5651 - acc: 0.7003 - val_loss: 0.4878 - val_precision: 0.9167 - val_recall: 0.6471 - val_acc: 0.8142\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5543 - precision: 0.6928 - recall: 0.5311 - acc: 0.7151 - val_loss: 0.4798 - val_precision: 0.8333 - val_recall: 0.6863 - val_acc: 0.7965\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5609 - precision: 0.7202 - recall: 0.5982 - acc: 0.7122 - val_loss: 0.4741 - val_precision: 0.8182 - val_recall: 0.7059 - val_acc: 0.7965\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5506 - precision: 0.6542 - recall: 0.6380 - acc: 0.7003 - val_loss: 0.4727 - val_precision: 0.8780 - val_recall: 0.7059 - val_acc: 0.8230\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5376 - precision: 0.7652 - recall: 0.6933 - acc: 0.7567 - val_loss: 0.4718 - val_precision: 0.8947 - val_recall: 0.6667 - val_acc: 0.8142\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5382 - precision: 0.6857 - recall: 0.5728 - acc: 0.7240 - val_loss: 0.4697 - val_precision: 0.9189 - val_recall: 0.6667 - val_acc: 0.8230\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 11ms/step - loss: 0.5057 - precision: 0.7247 - recall: 0.5652 - acc: 0.7389 - val_loss: 0.4608 - val_precision: 0.8571 - val_recall: 0.7059 - val_acc: 0.8142\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5295 - precision: 0.6967 - recall: 0.6528 - acc: 0.7389 - val_loss: 0.4531 - val_precision: 0.8444 - val_recall: 0.7451 - val_acc: 0.8230\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5199 - precision: 0.7438 - recall: 0.6940 - acc: 0.7656 - val_loss: 0.4491 - val_precision: 0.8571 - val_recall: 0.7059 - val_acc: 0.8142\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.5118 - precision: 0.7587 - recall: 0.6575 - acc: 0.7507 - val_loss: 0.4459 - val_precision: 0.9211 - val_recall: 0.6863 - val_acc: 0.8319\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5449 - precision: 0.6900 - recall: 0.6321 - acc: 0.7359 - val_loss: 0.4403 - val_precision: 0.9000 - val_recall: 0.7059 - val_acc: 0.8319\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.5066 - precision: 0.7495 - recall: 0.6947 - acc: 0.7596 - val_loss: 0.4324 - val_precision: 0.8261 - val_recall: 0.7451 - val_acc: 0.8142\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4982 - precision: 0.7478 - recall: 0.6977 - acc: 0.7537 - val_loss: 0.4291 - val_precision: 0.8372 - val_recall: 0.7059 - val_acc: 0.8053\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5066 - precision: 0.7579 - recall: 0.6023 - acc: 0.7507 - val_loss: 0.4243 - val_precision: 0.8372 - val_recall: 0.7059 - val_acc: 0.8053\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5126 - precision: 0.7403 - recall: 0.7076 - acc: 0.7923 - val_loss: 0.4237 - val_precision: 0.8780 - val_recall: 0.7059 - val_acc: 0.8230\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.5041 - precision: 0.7202 - recall: 0.6698 - acc: 0.7567 - val_loss: 0.4216 - val_precision: 0.9000 - val_recall: 0.7059 - val_acc: 0.8319\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.5271 - precision: 0.7069 - recall: 0.6251 - acc: 0.7448 - val_loss: 0.4208 - val_precision: 0.9000 - val_recall: 0.7059 - val_acc: 0.8319\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4868 - precision: 0.7485 - recall: 0.6856 - acc: 0.7745 - val_loss: 0.4139 - val_precision: 0.7917 - val_recall: 0.7451 - val_acc: 0.7965\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4832 - precision: 0.7199 - recall: 0.7137 - acc: 0.7567 - val_loss: 0.4105 - val_precision: 0.7917 - val_recall: 0.7451 - val_acc: 0.7965\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5091 - precision: 0.7171 - recall: 0.6740 - acc: 0.7537 - val_loss: 0.4117 - val_precision: 0.9000 - val_recall: 0.7059 - val_acc: 0.8319\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4741 - precision: 0.7373 - recall: 0.6878 - acc: 0.7656 - val_loss: 0.4142 - val_precision: 0.9211 - val_recall: 0.6863 - val_acc: 0.8319\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5056 - precision: 0.7331 - recall: 0.6263 - acc: 0.7478 - val_loss: 0.4102 - val_precision: 0.9231 - val_recall: 0.7059 - val_acc: 0.8407\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4893 - precision: 0.7395 - recall: 0.6667 - acc: 0.7715 - val_loss: 0.4028 - val_precision: 0.8837 - val_recall: 0.7451 - val_acc: 0.8407\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4740 - precision: 0.8344 - recall: 0.6963 - acc: 0.7982 - val_loss: 0.3985 - val_precision: 0.8478 - val_recall: 0.7647 - val_acc: 0.8319\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4986 - precision: 0.7431 - recall: 0.6005 - acc: 0.7389 - val_loss: 0.3988 - val_precision: 0.8636 - val_recall: 0.7451 - val_acc: 0.8319\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4869 - precision: 0.7194 - recall: 0.6672 - acc: 0.7567 - val_loss: 0.4043 - val_precision: 0.9000 - val_recall: 0.7059 - val_acc: 0.8319\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5004 - precision: 0.7887 - recall: 0.6541 - acc: 0.7774 - val_loss: 0.4062 - val_precision: 0.9231 - val_recall: 0.7059 - val_acc: 0.8407\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4776 - precision: 0.7919 - recall: 0.7012 - acc: 0.7745 - val_loss: 0.3982 - val_precision: 0.8605 - val_recall: 0.7255 - val_acc: 0.8230\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4835 - precision: 0.7276 - recall: 0.6956 - acc: 0.7774 - val_loss: 0.3982 - val_precision: 0.8605 - val_recall: 0.7255 - val_acc: 0.8230\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4972 - precision: 0.7374 - recall: 0.7239 - acc: 0.7596 - val_loss: 0.3979 - val_precision: 0.8810 - val_recall: 0.7255 - val_acc: 0.8319\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4686 - precision: 0.7583 - recall: 0.7602 - acc: 0.7804 - val_loss: 0.3941 - val_precision: 0.8667 - val_recall: 0.7647 - val_acc: 0.8407\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4793 - precision: 0.7836 - recall: 0.7123 - acc: 0.8101 - val_loss: 0.3906 - val_precision: 0.8667 - val_recall: 0.7647 - val_acc: 0.8407\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4943 - precision: 0.6842 - recall: 0.7085 - acc: 0.7656 - val_loss: 0.3926 - val_precision: 0.8864 - val_recall: 0.7647 - val_acc: 0.8496\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4642 - precision: 0.7833 - recall: 0.7472 - acc: 0.7982 - val_loss: 0.3904 - val_precision: 0.8837 - val_recall: 0.7451 - val_acc: 0.8407\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4740 - precision: 0.7482 - recall: 0.7197 - acc: 0.7864 - val_loss: 0.3896 - val_precision: 0.8636 - val_recall: 0.7451 - val_acc: 0.8319\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4723 - precision: 0.7412 - recall: 0.7015 - acc: 0.7864 - val_loss: 0.3890 - val_precision: 0.8636 - val_recall: 0.7451 - val_acc: 0.8319\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4527 - precision: 0.7694 - recall: 0.7433 - acc: 0.7864 - val_loss: 0.3876 - val_precision: 0.8837 - val_recall: 0.7451 - val_acc: 0.8407\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4786 - precision: 0.7563 - recall: 0.6650 - acc: 0.7745 - val_loss: 0.3816 - val_precision: 0.8667 - val_recall: 0.7647 - val_acc: 0.8407\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4701 - precision: 0.7353 - recall: 0.7802 - acc: 0.7923 - val_loss: 0.3828 - val_precision: 0.8667 - val_recall: 0.7647 - val_acc: 0.8407\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4789 - precision: 0.6599 - recall: 0.7028 - acc: 0.7507 - val_loss: 0.3842 - val_precision: 0.8667 - val_recall: 0.7647 - val_acc: 0.8407\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4524 - precision: 0.7640 - recall: 0.7345 - acc: 0.7923 - val_loss: 0.3803 - val_precision: 0.8511 - val_recall: 0.7843 - val_acc: 0.8407\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.4660 - precision: 0.7184 - recall: 0.6831 - acc: 0.7596 - val_loss: 0.3803 - val_precision: 0.8667 - val_recall: 0.7647 - val_acc: 0.8407\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4668 - precision: 0.7774 - recall: 0.7146 - acc: 0.7953 - val_loss: 0.3850 - val_precision: 0.8636 - val_recall: 0.7451 - val_acc: 0.8319\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4527 - precision: 0.7730 - recall: 0.6873 - acc: 0.7774 - val_loss: 0.3868 - val_precision: 0.8750 - val_recall: 0.6863 - val_acc: 0.8142\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4510 - precision: 0.7671 - recall: 0.6727 - acc: 0.7804 - val_loss: 0.3869 - val_precision: 0.9000 - val_recall: 0.7059 - val_acc: 0.8319\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4602 - precision: 0.8536 - recall: 0.7216 - acc: 0.7923 - val_loss: 0.3841 - val_precision: 0.8780 - val_recall: 0.7059 - val_acc: 0.8230\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4758 - precision: 0.7263 - recall: 0.6433 - acc: 0.7656 - val_loss: 0.3758 - val_precision: 0.8696 - val_recall: 0.7843 - val_acc: 0.8496\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4588 - precision: 0.7634 - recall: 0.7213 - acc: 0.7745 - val_loss: 0.3758 - val_precision: 0.8696 - val_recall: 0.7843 - val_acc: 0.8496\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4464 - precision: 0.7636 - recall: 0.7507 - acc: 0.7804 - val_loss: 0.3854 - val_precision: 0.8605 - val_recall: 0.7255 - val_acc: 0.8230\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4474 - precision: 0.8140 - recall: 0.7640 - acc: 0.8071 - val_loss: 0.3897 - val_precision: 0.8974 - val_recall: 0.6863 - val_acc: 0.8230\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4433 - precision: 0.8115 - recall: 0.6972 - acc: 0.8101 - val_loss: 0.3772 - val_precision: 0.8667 - val_recall: 0.7647 - val_acc: 0.8407\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4241 - precision: 0.7997 - recall: 0.7334 - acc: 0.8012 - val_loss: 0.3652 - val_precision: 0.8511 - val_recall: 0.7843 - val_acc: 0.8407\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4211 - precision: 0.7567 - recall: 0.8360 - acc: 0.8042 - val_loss: 0.3696 - val_precision: 0.8667 - val_recall: 0.7647 - val_acc: 0.8407\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4270 - precision: 0.7552 - recall: 0.7280 - acc: 0.7953 - val_loss: 0.3738 - val_precision: 0.8810 - val_recall: 0.7255 - val_acc: 0.8319\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4444 - precision: 0.7855 - recall: 0.7168 - acc: 0.7923 - val_loss: 0.3682 - val_precision: 0.8667 - val_recall: 0.7647 - val_acc: 0.8407\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4240 - precision: 0.7633 - recall: 0.7706 - acc: 0.8101 - val_loss: 0.3663 - val_precision: 0.8667 - val_recall: 0.7647 - val_acc: 0.8407\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4369 - precision: 0.7620 - recall: 0.7412 - acc: 0.7864 - val_loss: 0.3635 - val_precision: 0.8667 - val_recall: 0.7647 - val_acc: 0.8407\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4271 - precision: 0.8348 - recall: 0.7868 - acc: 0.8279 - val_loss: 0.3737 - val_precision: 0.8636 - val_recall: 0.7451 - val_acc: 0.8319\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4142 - precision: 0.8154 - recall: 0.7551 - acc: 0.8309 - val_loss: 0.3866 - val_precision: 0.8947 - val_recall: 0.6667 - val_acc: 0.8142\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4369 - precision: 0.8021 - recall: 0.6980 - acc: 0.7893 - val_loss: 0.3711 - val_precision: 0.8864 - val_recall: 0.7647 - val_acc: 0.8496\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4192 - precision: 0.7873 - recall: 0.7009 - acc: 0.7893 - val_loss: 0.3559 - val_precision: 0.8667 - val_recall: 0.7647 - val_acc: 0.8407\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4021 - precision: 0.7772 - recall: 0.7919 - acc: 0.8279 - val_loss: 0.3511 - val_precision: 0.8696 - val_recall: 0.7843 - val_acc: 0.8496\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4337 - precision: 0.7090 - recall: 0.7822 - acc: 0.7804 - val_loss: 0.3554 - val_precision: 0.8667 - val_recall: 0.7647 - val_acc: 0.8407\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4310 - precision: 0.7622 - recall: 0.7396 - acc: 0.7864 - val_loss: 0.3719 - val_precision: 0.9048 - val_recall: 0.7451 - val_acc: 0.8496\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4274 - precision: 0.8633 - recall: 0.7364 - acc: 0.8338 - val_loss: 0.3724 - val_precision: 0.9000 - val_recall: 0.7059 - val_acc: 0.8319\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4384 - precision: 0.8514 - recall: 0.6734 - acc: 0.7893 - val_loss: 0.3534 - val_precision: 0.8696 - val_recall: 0.7843 - val_acc: 0.8496\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4164 - precision: 0.8020 - recall: 0.8180 - acc: 0.8249 - val_loss: 0.3549 - val_precision: 0.8696 - val_recall: 0.7843 - val_acc: 0.8496\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4250 - precision: 0.7523 - recall: 0.7273 - acc: 0.7982 - val_loss: 0.3729 - val_precision: 0.9048 - val_recall: 0.7451 - val_acc: 0.8496\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4187 - precision: 0.8119 - recall: 0.6773 - acc: 0.7953 - val_loss: 0.3758 - val_precision: 0.9070 - val_recall: 0.7647 - val_acc: 0.8584\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4218 - precision: 0.8069 - recall: 0.6598 - acc: 0.7982 - val_loss: 0.3618 - val_precision: 0.8667 - val_recall: 0.7647 - val_acc: 0.8407\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4181 - precision: 0.7822 - recall: 0.7952 - acc: 0.8160 - val_loss: 0.3582 - val_precision: 0.8667 - val_recall: 0.7647 - val_acc: 0.8407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f601cff5a00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 128 #  128\n",
    "EPOCHS = 100\n",
    "\n",
    "classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras_metrics.precision(), keras_metrics.recall(), 'acc'])\n",
    "        \n",
    "    # Fit the model    \n",
    "    #classifier.fit(x_train, y_train, validation_data = (x_test, y_test), epochs=150, verbose=0) \n",
    "\n",
    "classifier.fit(X_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          validation_data = (X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.35819998383522034\n",
      "Test accuracy: 0.7876259088516235\n"
     ]
    }
   ],
   "source": [
    "score = classifier.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [1 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1\n",
      " 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 0\n",
      " 1 0 0 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 0 1 1\n",
      " 0 0]\n",
      "Test [0. 1.]\n",
      "Test 1\n",
      "Test [1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0\n",
      " 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 0 1 1\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Predicting test labels    \n",
    "\n",
    "pred = classifier.predict(X_test)\n",
    "pred = pred.round().astype(int)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "#pred = pred.reshape(y_test.shape)\n",
    "\n",
    "print(\"Predicted:\", pred)\n",
    "print(\"Test\", y_test[0])\n",
    "print(\"Test\", y_test[0].argmax(0))\n",
    "\n",
    "# one-hot to simple test format\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "print(\"Test\", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: ['PA' 'HC' 'PA' 'PA' 'PA' 'HC' 'PA' 'PA' 'PA' 'HC' 'PA' 'HC' 'HC' 'PA'\n",
      " 'PA' 'PA' 'HC' 'HC' 'PA' 'PA' 'HC' 'HC' 'PA' 'PA' 'PA' 'HC' 'PA' 'PA'\n",
      " 'PA' 'PA' 'PA' 'HC' 'PA' 'HC' 'PA' 'PA' 'PA' 'PA' 'PA' 'PA' 'PA' 'HC'\n",
      " 'PA' 'HC' 'PA' 'HC' 'HC' 'HC' 'PA' 'PA' 'PA' 'PA' 'HC' 'HC' 'PA' 'PA'\n",
      " 'PA' 'PA' 'PA' 'HC' 'HC' 'PA' 'HC' 'HC' 'PA' 'HC' 'PA' 'PA' 'PA' 'HC'\n",
      " 'PA' 'PA' 'PA' 'HC' 'PA' 'HC' 'HC' 'PA' 'PA' 'HC' 'HC' 'HC' 'PA' 'PA'\n",
      " 'HC' 'HC' 'HC' 'PA' 'HC' 'PA' 'PA' 'PA' 'HC' 'PA' 'HC' 'PA' 'PA' 'HC'\n",
      " 'PA' 'PA' 'PA' 'PA' 'PA' 'HC' 'HC' 'PA' 'HC' 'HC' 'HC' 'PA' 'PA' 'HC'\n",
      " 'HC']\n",
      "Test ['PA' 'HC' 'PA' 'PA' 'PA' 'PA' 'PA' 'HC' 'PA' 'PA' 'PA' 'PA' 'HC' 'HC'\n",
      " 'HC' 'PA' 'HC' 'HC' 'PA' 'PA' 'HC' 'HC' 'PA' 'HC' 'PA' 'HC' 'PA' 'PA'\n",
      " 'PA' 'PA' 'PA' 'PA' 'PA' 'HC' 'PA' 'PA' 'PA' 'PA' 'PA' 'PA' 'PA' 'HC'\n",
      " 'PA' 'HC' 'PA' 'HC' 'HC' 'PA' 'PA' 'PA' 'PA' 'HC' 'HC' 'HC' 'PA' 'HC'\n",
      " 'PA' 'PA' 'PA' 'HC' 'HC' 'PA' 'HC' 'HC' 'HC' 'HC' 'PA' 'PA' 'PA' 'HC'\n",
      " 'PA' 'PA' 'PA' 'HC' 'PA' 'HC' 'HC' 'PA' 'HC' 'HC' 'HC' 'HC' 'PA' 'HC'\n",
      " 'HC' 'HC' 'HC' 'PA' 'HC' 'PA' 'PA' 'HC' 'HC' 'HC' 'HC' 'PA' 'PA' 'HC'\n",
      " 'PA' 'PA' 'PA' 'PA' 'HC' 'HC' 'PA' 'PA' 'HC' 'HC' 'HC' 'PA' 'PA' 'HC'\n",
      " 'HC']\n"
     ]
    }
   ],
   "source": [
    "pred = pred.astype('str')\n",
    "y_test = y_test.astype('str')\n",
    "pred[pred == '0'] = decode[0]\n",
    "pred[pred == '1'] = decode[1]\n",
    "y_test[y_test == '0'] = decode[0]\n",
    "y_test[y_test == '1'] = decode[1]\n",
    "\n",
    "print(\"Predicted:\", pred)\n",
    "print(\"Test\", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True       HC  PA  All\n",
      "Predicted             \n",
      "HC         39   6   45\n",
      "PA         12  56   68\n",
      "All        51  62  113\n"
     ]
    }
   ],
   "source": [
    "y_actu = pd.Series(y_test, name='True')\n",
    "y_pred = pd.Series(pred, name='Pred')\n",
    "df_confusion = pd.crosstab(y_pred, y_actu, colnames=['True'], rownames=['Predicted'], margins=True)\n",
    "print(\"\")\n",
    "print(df_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39  6]\n",
      " [12 56]]\n"
     ]
    }
   ],
   "source": [
    "cm_final = df_confusion.iloc[0:-1].values\n",
    "\n",
    "cm_final = cm_final[:,[0,1]]\n",
    "print(cm_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Statistical measures calculated from Confusion Matrix #\n",
    "#########################################################\n",
    "import math\n",
    "\n",
    "# (tp + tn) / (tp + fp + tn + fn)\n",
    "def get_accuracy(mx):\n",
    "    [tp, fp], [fn, tn] = mx\n",
    "    #print([tp, fp], [fn, tn])\n",
    "    return (tp + tn) / (tp + fp + tn + fn)\n",
    "\n",
    "# sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "# tp / (tp + fn)\n",
    "def get_recall(mx):\n",
    "    [tp, fp], [fn, tn] = mx\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "# precision or positive predictive value (PPV)\n",
    "# tp / (tp + fp)\n",
    "def get_precision(mx):\n",
    "    [tp, fp], [fn, tn] = mx\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "# harmonic mean of precision and sensitivity\n",
    "# 2*((precision*recall)/(precision+recall))\n",
    "def get_f1score(mx):\n",
    "    return 2*((get_precision(mx)*get_recall(mx))/(get_precision(mx)+get_recall(mx)))\n",
    "\n",
    "# specificity, selectivity or true negative rate (TNR)\n",
    "# \n",
    "def get_specificity(mx):\n",
    "    [tp, fp], [fn, tn] = mx\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def get_sensitivity(mx):\n",
    "    [tp, fp], [fn, tn] = mx\n",
    "    return tn/(tn+fp)\n",
    "\n",
    "def get_MCC(mx):\n",
    "    # Matthews Correlation Coefficient (MCC)\n",
    "    [tp, fp], [fn, tn] = mx\n",
    "    \n",
    "    return (tp*tn-fp*fn)/math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________\n",
      "\n",
      "Accuracy: 0.840708\n",
      "Matthews Correlation Coefficient: 0.678973\n",
      "\n",
      "For class HC: \n",
      "\n",
      "Recall: 0.764706\n",
      "Precision: 0.866667\n",
      "F1 score: 0.812500\n",
      "Specificity: 0.764706\n",
      "Sensitivity: 0.903226\n",
      "\n",
      "For class PA: \n",
      "\n",
      "Recall: 0.903226\n",
      "Precision: 0.823529\n",
      "F1 score: 0.861538\n",
      "Specificity: 0.903226\n",
      "Sensitivity: 0.764706\n"
     ]
    }
   ],
   "source": [
    "# Calculating statistical measures from Confusion Matrix\n",
    "mx = cm_final\n",
    "print(\"__________________________________\")\n",
    "print(\"\")\n",
    "print('Accuracy: %f' % get_accuracy(mx))\n",
    "print('Matthews Correlation Coefficient: %f' % get_MCC(mx))\n",
    "print(\"\")\n",
    "\n",
    "print(\"For class {}: \".format(df_confusion.columns[0]) )\n",
    "print(\"\")\n",
    "print('Recall: %f' % get_recall(mx))\n",
    "print('Precision: %f' % get_precision(mx))\n",
    "print('F1 score: %f' % get_f1score(mx))\n",
    "print('Specificity: %f' % get_specificity(mx))\n",
    "print('Sensitivity: %f' % get_sensitivity(mx))\n",
    "\n",
    "[tn, fp], [fn, tp] = mx\n",
    "mx_ = [tp, fn],[fp, tn]\n",
    "print(\"\")\n",
    "print(\"For class {}: \".format(df_confusion.columns[1]) )\n",
    "print(\"\")\n",
    "print('Recall: %f' % get_recall(mx_))\n",
    "print('Precision: %f' % get_precision(mx_))\n",
    "print('F1 score: %f' % get_f1score(mx_))\n",
    "print('Specificity: %f' % get_specificity(mx_))\n",
    "print('Sensitivity: %f' % get_sensitivity(mx_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
